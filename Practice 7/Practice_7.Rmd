---
title: "Practice Problems 7"
author: "Jordan Lian"
date: "3/28/2021"
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(tidyverse)
```

```{r eval=FALSE}
library(tidyverse)
```

## Problem 1
Build an R Notebook of the concrete strength example in the textbook on pages 232 to 239. Show each step and add appropriate documentation.

### 1. Collecting Data
http://archive.ics.uci.edu/ml

### 2. Exploring and preparing the data
```{r warning=FALSE}
# Load in data set
concrete <- read.csv("concrete.csv")
str(concrete)

# Normalize function
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# Normalize data
concrete_norm <- as.data.frame(lapply(concrete, normalize))

# Confirm that normalization worked
summary(concrete_norm$strength)
summary(concrete$strength)

# Split into training and test
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
```

### 3. Training a model on the data
```{r}
# Train the simplest multiplayer feedforward network
library(neuralnet)
concrete_model <- neuralnet(strength ~ cement + slag + ash + water + superplastic + 
                              coarseagg + fineagg + age, data = concrete_train)

# Visualize network topology
plot(concrete_model, rep="best")
```

In this simple model, there is one input node for each of the eight features, followed by a single hidden node and a single output node that predicts the concrete strength. The weights for each of the connections are also depicted, as are the bias terms (indicated by the nodes labeled with the number 1). The bias terms are numeric constants that allow the value at the indicated nodes to be shifted upward or downward, much like the intercept in a linear equation.

At the bottom of the figure, R reports the number of training steps and an error measure called the Sum of Squared Errors (SSE), which as you might expect, is the sum of the squared predicted minus actual values. A lower SSE implies better predictive performance. This is helpful for estimating the model's performance on the training data, but tells us little about how it will perform on unseen data.

### 4. Evaluating model performance
```{r}
model_results <- compute(concrete_model, concrete_test[1:8])
predicted_strength <- model_results$net.result
cor(predicted_strength, concrete_test$strength)
```

### 5. Improving model performance
```{r}
# Change hidden value
concrete_model2 <- neuralnet(strength ~ cement + slag + ash + water + superplastic +
                               coarseagg + fineagg + age, data = concrete_train, hidden = 5)
plot(concrete_model2, rep="best")

model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
```

## Problem 2
Build an R Notebook of the optical character recognition example in the textbook on pages 249 to 257. Show each step and add appropriate documentation.

### 1. Collecting Data
http://archive.ics.uci.edu/ml

### 2. Exploring and preparing the data
```{r warning=FALSE}
# Load in dataset
letters <- read.csv("letterdata.csv")
str(letters)

# Make certain columns factors
col_names <- names(letters)
letters[,col_names] <- lapply(letters[,col_names] , factor)

# Split into training/test datasets
letters_train <- letters[1:16000, ]
letters_test  <- letters[16001:20000, ]
```

### 3. Training a model on the data
```{r}
library(caret)
library(kernlab)
library(e1071)
library(klaR)

# Use ksvm()
letter_classifier <- ksvm(letter ~ ., data = letters_train, kernel = "vanilladot")
letter_classifier
```

In this simple model, there is one input node for each of the eight features, followed by a single hidden node and a single output node that predicts the concrete strength. The weights for each of the connections are also depicted, as are the bias terms (indicated by the nodes labeled with the number 1). The bias terms are numeric constants that allow the value at the indicated nodes to be shifted upward or downward, much like the intercept in a linear equation.

At the bottom of the figure, R reports the number of training steps and an error measure called the Sum of Squared Errors (SSE), which as you might expect, is the sum of the squared predicted minus actual values. A lower SSE implies better predictive performance. This is helpful for estimating the model's performance on the training data, but tells us little about how it will perform on unseen data.

### 4. Evaluating model performance
```{r eval=FALSE}
# Predictions
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)

# Table
table(letter_predictions, letters_test$letter)

# Table and proportion
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
```

### 5. Improving model performance
```{r}
# Change kernel
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train, kernel = "rbfdot")
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test)

# Get table and percentages
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
prop.table(table(agreement_rbf))
```

## Problem 3
Build an R Notebook of the grocery store transactions example in the textbook on pages 266 to 284. Show each step and add appropriate documentation.

### 1. Collecting Data
http://archive.ics.uci.edu/ml

### 2. Exploring and preparing the data

### Data Preparation - creating a sparse matrix for transaction data
```{r warning=FALSE}
# Load in dataset
library(arules)
groceries <- read.transactions("groceries.csv", sep = ",")
summary(groceries)
inspect(groceries[1:5])
itemFrequency(groceries[, 1:3])
```

### Visualizing item support - item frequency plots
```{r}
itemFrequencyPlot(groceries, support = 0.1)
itemFrequencyPlot(groceries, topN = 20)
```

### Visualizing the transaction data - plotting the sparse matrix
```{r}
image(groceries[1:5])
image(sample(groceries, 100))
```

### 3. Training a model on the data
```{r}
# Use apriori()
apriori(groceries)
groceryrules <- apriori(groceries, parameter = list(support = 0.006, confidence = 0.25, minlen = 2))
groceryrules
```

The Sum of Squared Errors (SSE) is the sum of the squared predicted minus actual values. A lower SSE implies better predictive performance. This is helpful for estimating the model's performance on the training data, but it doesn't tell how it will perform on unseen data.

### 4. Evaluating model performance
```{r}
summary(groceryrules)
inspect(groceryrules[1:3])
```

The lift of a rule measures how much more likely one item or item set is purchased relative to its typical rate of purchase, given that you know another item or item set has been purchased. This is defined by the following equation:

$$lift (X \rightarrow Y) = \frac{confidence(X \rightarrow Y)}{support(Y)}$$
Unlike confidence where the item order matters, $lift(X \rightarrow Y)$ is the same as $lift(Y \rightarrow X)$.

### 5. Improving model performance

### Sorting the set of association rules
```{r}
inspect(sort(groceryrules, by = "lift")[1:5])
```

### Taking subsets of association rules
```{r}
berryrules <- subset(groceryrules, items %in% "berries")
inspect(berryrules)
```

### Saving association rules to a file or data frame
```{r}
write(groceryrules, file = "groceryrules.csv", sep = ",", quote = TRUE, row.names = FALSE)
groceryrules_df <- as(groceryrules, "data.frame")
str(groceryrules_df)
```